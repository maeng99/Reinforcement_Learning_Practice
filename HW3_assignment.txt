minimal RL implementation 코드 중 아래의 DQN 코드를 기반으로 다음의 내용들을 수행하시오.
(https://github.com/seungeunrho/minimalRL/blob/master/dqn.py)

(1) 위 코드를 Colab에 옮겨서, 적절히 셀 별로 코드를 분할하고 정리해서 실행하라. 
처음 셀에서 다음 명령을 통해 패키지 설치 후 실행.
!pip3 install gymnasium[classic_control]
이 과정에서 코드의 주요 부분에 주석을 달아 코드를 이해해 가면서 정리.
학습이 오류 없이 실행되는 지 여부만 체크, 끝까지 돌릴 필요는 없음.

(2) 그 뒤, runtime을 GPU 지원으로 바꿔서 device = 'cuda'를 이용하여 모델 학습을 GPU에서 돌아가게 만든다.
학습을 끝까지 수행하되, 기존 코드의 hyperparameter (반복 횟수, learning rate, epsilon 설정, target network를 업데이트 시키는 빈도 등...)를
바꿔가면서 적절한 셋업을 찾기 (최적의 결과를 구하기는 어렵기 때문에, 학습 속도가 충분히 빠르고 여러번 돌렸을 때 결과가 안정적으로 나오는지 확인).

(3) 이제, 코드의 일부분을 수정하여 Replay Buffer에서 batch 크기만큼 샘플링을 하는 대신,
batch 크기만큼의 연속적인 transition을 입력으로 받도록 해서 학습을 돌리고, 결과를 비교하라.

(4) 위의 (3)번에서 수정한 내용이 아닌, (2)번까지 수행한 원래의 코드를 기준으로 이번에는 target network를 사용하지 않고,
하나의 Q-network로 target과 policy를 모두 담당하도록 코드를 수정하라. 학습을 돌리고, 결과를 비교하라.

(5) 이번에는 위의 (3)번과 (4)번 변경내용을 같이 적용해서 학습을 돌리고, 결과를 비교하라.
이렇게 하는 방식이 naive DQN.

(6) 종합적으로 (2)번에서 수행한 원래의 DQN 모델, (3)에서 수행한 Replay Buffer 미 사용,
(4)에서 수행한 Target Network 미 사용, (5)에서 수행한 naive DQN의 결과를 비교하고, 논의하라.
