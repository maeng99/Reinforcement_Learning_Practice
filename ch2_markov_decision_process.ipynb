{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PkYdSJS79s3I","executionInfo":{"status":"ok","timestamp":1727247384042,"user_tz":-540,"elapsed":36595,"user":{"displayName":"맹의현","userId":"03582999303334923096"}},"outputId":"aca6dc8d-3b67-40ab-f8de-3d48ada22015"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"oPBs-Eq19PmK","executionInfo":{"status":"ok","timestamp":1727247415562,"user_tz":-540,"elapsed":343,"user":{"displayName":"맹의현","userId":"03582999303334923096"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"TBeE1KjW9PmO"},"source":["# 3. Markov Decision Process (MDP)"]},{"cell_type":"markdown","metadata":{"id":"pXefkDA_9PmP"},"source":["### 3-1. Markov Reward Process (MRP) for a given action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-gAwGb09PmQ"},"outputs":[],"source":["# We assume that action is taken only at state S0 (initial state)\n","# Action space = {Left, Right}"]},{"cell_type":"markdown","metadata":{"id":"rAdBD8oe9PmR"},"source":["<img src=\"MDP_1.png\" width=400>"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ktMIOisn9PmR","executionInfo":{"status":"ok","timestamp":1727247509909,"user_tz":-540,"elapsed":350,"user":{"displayName":"맹의현","userId":"03582999303334923096"}}},"outputs":[],"source":["# state transition probability matrix for a=Left\n","# probability to move from state i (row) to state j (column)\n","PL = [[0.0, 0.9, 0.1, 0.0],\n","      [0.0, 1.0, 0.0, 0.0],\n","      [0.0, 0.0, 0.0, 1.0],\n","      [0.0, 0.0, 0.0, 1.0]]\n","\n","# state transition probability matrix for a=Right\n","# probability to move from state i (row) to state j (column)\n","PR = [[0.0, 0.1, 0.9, 0.0],\n","      [0.0, 1.0, 0.0, 0.0],\n","      [0.0, 0.0, 0.0, 1.0],\n","      [0.0, 0.0, 0.0, 1.0]]\n","\n","# reward matrix\n","# immediate reward when moving from state i (row) to state j (column)\n","# reward matrix can be also dependent on action,\n","# but in this formulation, we assume that reward is defined by transition between states\n","R = [[0.0, 1.0, 2.0, 0.0],\n","     [0.0, 0.0, 0.0, 0.0],\n","     [0.0, 0.0, 0.0, -10.0],\n","     [0.0, 0.0, 0.0, 0.0]]"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"fZtYFwRw9PmS","executionInfo":{"status":"ok","timestamp":1727247528099,"user_tz":-540,"elapsed":388,"user":{"displayName":"맹의현","userId":"03582999303334923096"}}},"outputs":[],"source":["def next_state(s, P):\n","    '''\n","    현재 상태 s와 state transition matrix P가 주어질 때,\n","    다음 상태 s를 반환하는 함수\n","\n","    input:\n","    s: 0에서 n-1까지의 값을 가질 수 있는 정수 (상태)\n","    P: n x n 행렬 (state transtion matrix)\n","\n","    output:\n","    next_s: 확률에 의해 결정된 다음 상태\n","    '''\n","\n","    n = len(P) # P 행렬의 행의 갯수 (= 상태의 갯수)\n","\n","    # cumulative sum of the state transition matrix\n","    csP = np.cumsum(P, axis=1) # sum along rows\n","    zero_vec = np.zeros((n, 1)) # a column vector (nx1 matrix) with zero elements\n","    csP = np.concatenate((zero_vec, csP), axis=1) # concatenate two matrices\n","\n","    prob = np.random.uniform()\n","    for k in range(n):\n","        if (prob >= csP[s][k]) and (prob < csP[s][k+1]):\n","            next_s = k\n","            break\n","\n","    return next_s"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"YLgQhV2K9PmS","executionInfo":{"status":"ok","timestamp":1727247641289,"user_tz":-540,"elapsed":383,"user":{"displayName":"맹의현","userId":"03582999303334923096"}}},"outputs":[],"source":["def sample_MRP(P, gamma, num_episodes=20, max_len=1000):\n","    '''\n","    function for sampling of Markov reward process for a given P matrix\n","\n","    input:\n","    P: state transition probability matrix for a given action\n","    gamma: discount factor\n","    num_episodes: number of samples\n","    max_len: maximum length of a single episode\n","\n","    output:\n","    list_episodes: a list of episodes\n","    list_rewards: a list of reward history\n","    list_return: a list of returns\n","    '''\n","\n","    # find terminal states from P\n","    terminal_states = []\n","    for row in range(len(P)):\n","        if P[row][row] == 1.0:\n","            terminal_states.append(row)\n","\n","    list_episodes = []\n","    list_rewards = []\n","    list_return = []\n","\n","    for i in range(num_episodes):\n","\n","        # start of a new episode\n","        s = 0 # initial state\n","        episode = [s]\n","        reward = []\n","        G = 0.0 # initialize return to 0 for each episode\n","        t = 0 # episode counter (time)\n","\n","        while (t < max_len) and (s not in terminal_states):\n","            next_s = next_state(s, P)\n","            r = gamma**t * R[s][next_s] # discounted reward at time t\n","            G += r\n","\n","            episode.append(next_s)\n","            reward.append(r)\n","            s = next_s # 다음 상태를 현재 상태로 업데이트\n","            t += 1\n","\n","        list_episodes.append(episode)\n","        list_rewards.append(reward)\n","        list_return.append(G)\n","\n","    return list_episodes, list_rewards, list_return"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"asrjQLLz9PmT","executionInfo":{"status":"ok","timestamp":1727247768266,"user_tz":-540,"elapsed":352,"user":{"displayName":"맹의현","userId":"03582999303334923096"}}},"outputs":[],"source":["# Markov reward process for action=Left\n","list_episodes, list_rewards, list_return = sample_MRP(PL, 1.0)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6aYm4Hv9PmU","executionInfo":{"status":"ok","timestamp":1727247770611,"user_tz":-540,"elapsed":323,"user":{"displayName":"맹의현","userId":"03582999303334923096"}},"outputId":"66a2f609-a2ed-4d21-e691-70f8fc9c6c5b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0, 1],\n"," [0, 1],\n"," [0, 2, 3],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 2, 3],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1]]"]},"metadata":{},"execution_count":8}],"source":["list_episodes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0fbjFid9PmU","outputId":"e07900e5-7045-44df-90fa-25beaa0bffb6"},"outputs":[{"data":{"text/plain":["[[1.0],\n"," [1.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [2.0, -10.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [2.0, -10.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0]]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["list_rewards"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81_XIYSh9PmU","outputId":"b098342b-57aa-40da-82de-481c6c7d182f"},"outputs":[{"data":{"text/plain":["[1.0,\n"," 1.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," -8.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," -8.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["list_return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sq2gYWwz9PmV"},"outputs":[],"source":["# Markov reward process for action=Right\n","list_episodes, list_rewards, list_return = sample_MRP(PR, 1.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUZ2MRSi9PmV","outputId":"2e747d4e-3708-457b-e768-4ab5dd4560f2"},"outputs":[{"data":{"text/plain":["[[0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 1],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 1],\n"," [0, 2, 3],\n"," [0, 2, 3]]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["list_episodes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6ZPi0nT9PmV","outputId":"55f2bec8-851a-44a6-cb35-97cd18accfd1"},"outputs":[{"data":{"text/plain":["[[2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [1.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [1.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0]]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["list_rewards"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKum_4eF9PmV","outputId":"eeb123a3-3783-4e0b-9ecb-aad534f9c786"},"outputs":[{"data":{"text/plain":["[-8.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," 1.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," 1.0,\n"," -8.0,\n"," -8.0]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["list_return"]},{"cell_type":"markdown","metadata":{"id":"kuzTPzj39PmW"},"source":["### 3-2. Policy and Markov Decision Process"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmrOGxyj9PmW"},"outputs":[],"source":["def policy(p_left):\n","    prob = np.random.uniform()\n","    if prob <= p_left:\n","        action = 'L'\n","    else:\n","        action = 'R'\n","    return action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsrjA_dj9PmW","outputId":"d95ee9ef-0a85-4774-e47e-ce883155bfc0"},"outputs":[{"data":{"text/plain":["['L', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'R', 'L']"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# sample policy 10 times\n","a = [policy(0.8) for _ in range(10)]\n","a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vrhig6G9PmW"},"outputs":[],"source":["def sample_MDP(p_left, gamma, num_episodes=20, max_len=1000):\n","    '''\n","    function for sampling of Markov decision process\n","\n","    input:\n","    p_left: policy probability to choose action \"Left(L)\"\n","    gamma: discount factor\n","    num_episodes: number of samples\n","    max_len: maximum length of a single episode\n","\n","    output:\n","    list_episodes: a list of episodes\n","    list_action: a list of actions\n","    list_rewards: a list of reward history\n","    list_return: a list of returns\n","    '''\n","\n","    list_episodes = []\n","    list_action = []\n","    list_rewards = []\n","    list_return = []\n","\n","    for i in range(num_episodes):\n","\n","        # sample action from policy and decides P matrix\n","        a = policy(p_left)\n","        P = PL if a=='L' else PR\n","\n","        # find terminal states from P\n","        terminal_states = []\n","        for row in range(len(P)):\n","            if P[row][row] == 1.0:\n","                terminal_states.append(row)\n","\n","        # start of a new episode\n","        s = 0 # initial state\n","        episode = [s]\n","        reward = []\n","        G = 0.0 # initialize return to 0 for each episode\n","        t = 0 # episode counter (time)\n","\n","        while (t < max_len) and (s not in terminal_states):\n","            next_s = next_state(s, P)\n","            r = gamma**t * R[s][next_s] # discounted reward at time t\n","            G += r\n","\n","            episode.append(next_s)\n","            reward.append(r)\n","            s = next_s # 다음 상태를 현재 상태로 업데이트\n","            t += 1\n","\n","        list_episodes.append(episode)\n","        list_action.append(a)\n","        list_rewards.append(reward)\n","        list_return.append(G)\n","\n","    return list_episodes, list_action, list_rewards, list_return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CnGjIGRD9PmW"},"outputs":[],"source":["p_left = 0.5 # \"Half\" policy\n","gamma = 1.0\n","list_episodes, list_action, list_rewards, list_return = sample_MDP(p_left, gamma)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoXlP5fP9PmW","outputId":"3e7aeb37-6e1f-4818-d3c5-fd8ec802e0d5"},"outputs":[{"data":{"text/plain":["['L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'R',\n"," 'L',\n"," 'R',\n"," 'L',\n"," 'L',\n"," 'R',\n"," 'L',\n"," 'R',\n"," 'R',\n"," 'L',\n"," 'R',\n"," 'R',\n"," 'R',\n"," 'L',\n"," 'R',\n"," 'L']"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["list_action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xpKhnVyF9PmX","outputId":"d2571657-c720-458d-f160-928a13b1ac0c"},"outputs":[{"data":{"text/plain":["[[0, 1],\n"," [0, 1],\n"," [0, 2, 3],\n"," [0, 1],\n"," [0, 2, 3],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 2, 3],\n"," [0, 1],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 1],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 2, 3],\n"," [0, 1]]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["list_episodes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mR7RAHr9PmX","outputId":"cced0988-ff71-4bea-f3f4-a2619f3f0917"},"outputs":[{"data":{"text/plain":["[[1.0],\n"," [1.0],\n"," [2.0, -10.0],\n"," [1.0],\n"," [2.0, -10.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [2.0, -10.0],\n"," [1.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [1.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [2.0, -10.0],\n"," [1.0]]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["list_rewards"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQBFIuPh9PmX","outputId":"0dd9b9f6-15a3-47ef-f89a-7060cb6a216c"},"outputs":[{"data":{"text/plain":["[1.0,\n"," 1.0,\n"," -8.0,\n"," 1.0,\n"," -8.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," -8.0,\n"," 1.0,\n"," -8.0,\n"," -8.0,\n"," 1.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," -8.0,\n"," 1.0]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["list_return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5JTCFbtN9PmX"},"outputs":[],"source":["p_left = 1.0 # \"Always Left\" policy\n","gamma = 1.0\n","list_episodes, list_action, list_rewards, list_return = sample_MDP(p_left, gamma)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUnQHOmq9PmX","outputId":"234ec367-93aa-4699-f82b-bcbf96aeaf46"},"outputs":[{"data":{"text/plain":["['L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L',\n"," 'L']"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["list_action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzCDg4kt9PmX","outputId":"fff36b53-1202-4d1d-97c4-b28aeac9a97e"},"outputs":[{"data":{"text/plain":["[[0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 2, 3],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1],\n"," [0, 1]]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["list_episodes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9EnWYWR9PmY","outputId":"73b4142c-8d96-4e1e-a206-fc4d2e32d14a"},"outputs":[{"data":{"text/plain":["[[1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [2.0, -10.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0],\n"," [1.0]]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["list_rewards"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3b9i4rf9PmY","outputId":"33626dae-d86d-4f52-f028-fbff2bca0aad"},"outputs":[{"data":{"text/plain":["[1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," -8.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["list_return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IylUs06j9PmY","outputId":"0b7febbd-983a-4785-c592-e731c264689f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Value of S0 for \"Always Left\" policy= 0.08776\n"]}],"source":["# Let's calculate a state-value of State 0 (S0) for \"Always Left\" policy\n","# This value is equal to the action-value of q(S0, L)\n","p_left = 1.0 # \"Always Left\" policy\n","gamma = 1.0\n","list_episodes, list_action, list_rewards, list_return = sample_MDP(p_left, gamma, num_episodes=100000)\n","value_s0 = np.mean(list_return)\n","print('Value of S0 for \"Always Left\" policy=', value_s0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbaZ41Pf9PmY","outputId":"ede3305a-b10e-45a7-f86e-baa403de08eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Value of S0 for \"Always Right\" policy= -7.10009\n"]}],"source":["# Let's calculate a state-value of State 0 (S0) for \"Always Right\" policy\n","# This value is equal to the action-value of q(S0, R)\n","p_left = 0.0 # \"Always Right\" policy\n","gamma = 1.0\n","list_episodes, list_action, list_rewards, list_return = sample_MDP(p_left, gamma, num_episodes=100000)\n","value_s0 = np.mean(list_return)\n","print('Value of S0 for \"Always Right\" policy=', value_s0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdO4wWHO9PmY","outputId":"ff540587-0e62-40f5-b497-1aeb2d151310"},"outputs":[{"name":"stdout","output_type":"stream","text":["Value of S0 for \"Half and Half\" policy= -3.50909\n"]}],"source":["# Let's calculate a state-value of State 0 (S0) for \"Half and Half\" policy\n","# This value is equal to 0.5 * q(S0, L) + 0.5 * q(S0, R)\n","p_left = 0.5 # \"Half and Half\" policy\n","gamma = 1.0\n","list_episodes, list_action, list_rewards, list_return = sample_MDP(p_left, gamma, num_episodes=100000)\n","value_s0 = np.mean(list_return)\n","print('Value of S0 for \"Half and Half\" policy=', value_s0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"izKR1Ci69PmZ","outputId":"d2f92424-8e64-42f2-c06a-2ab4e7b66360"},"outputs":[{"data":{"text/plain":["-3.503105"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["0.5 * 0.10135 + 0.5 * (-7.10756)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CYbB6E2G9PmZ"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"gymnasium","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}