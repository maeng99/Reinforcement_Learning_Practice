{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMHr8GSqMy5dUSV1+ygYmNm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# (1), (2)"],"metadata":{"id":"AzTEY_rKJEq3"}},{"cell_type":"code","source":["!pip3 install gymnasium[classic_control]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z1Y1eQz9XH34","executionInfo":{"status":"ok","timestamp":1732604360675,"user_tz":-540,"elapsed":5526,"user":{"displayName":"맹의현","userId":"03582999303334923096"}},"outputId":"de4a0c3c-8ee7-4592-8098-eec35fa0b1cb"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium[classic_control]\n","  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (4.12.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium[classic_control])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (2.6.1)\n","Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"]}]},{"cell_type":"markdown","source":["## Import"],"metadata":{"id":"AYfIEkDpbfIn"}},{"cell_type":"code","source":["import gym\n","import collections\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import itertools"],"metadata":{"id":"cPb23Mf8X8aF","executionInfo":{"status":"ok","timestamp":1732604396736,"user_tz":-540,"elapsed":4487,"user":{"displayName":"맹의현","userId":"03582999303334923096"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Hyperparameter 정의"],"metadata":{"id":"Xx-xBr8wbkAd"}},{"cell_type":"code","source":["#Hyperparameters\n","learning_rate = 0.001\n","gamma         = 0.98\n","buffer_limit  = 50000\n","batch_size    = 32\n","\n","# GPU 사용을 위한 device 설정\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gif3-Of2YAo2","executionInfo":{"status":"ok","timestamp":1732604396736,"user_tz":-540,"elapsed":8,"user":{"displayName":"맹의현","userId":"03582999303334923096"}},"outputId":"7691857e-6d80-4047-cc31-df7c2f8a7091"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","source":["## ReplayBuffer 클래스 정의"],"metadata":{"id":"4zgFeoJpbyHg"}},{"cell_type":"code","source":["## 에이전트의 경험을 저장하고 샘플링하는 역할\n","class ReplayBuffer():\n","    def __init__(self):\n","        self.buffer = collections.deque(maxlen=buffer_limit)    # 경험 저장 공간 설정\n","\n","    def put(self, transition):\n","        self.buffer.append(transition)    # 새로운 경험인 transition을 저장 공간에 추가\n","\n","    def sample(self, n):\n","        mini_batch = random.sample(self.buffer, n)    # n(mini_batch의 크기)만큼 샘플링\n","        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n","\n","        for transition in mini_batch:\n","            s, a, r, s_prime, done_mask = transition\n","            s_lst.append(s)   # 현재 상태(state)\n","            a_lst.append([a])   # 행동(action)\n","            r_lst.append([r])   # 보상(reward)\n","            s_prime_lst.append(s_prime)   # 다음 상태\n","            done_mask_lst.append([done_mask])   # 종료 여부\n","\n","        # 텐서 형태로 반환 수행\n","        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n","               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n","               torch.tensor(done_mask_lst)\n","\n","    def size(self):\n","        return len(self.buffer)   # 저장 공간의 길이 반환"],"metadata":{"id":"4P_Qe69qa-Lk","executionInfo":{"status":"ok","timestamp":1732604396736,"user_tz":-540,"elapsed":5,"user":{"displayName":"맹의현","userId":"03582999303334923096"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Q-Network 클래스 정의"],"metadata":{"id":"NbE0Ajlnb573"}},{"cell_type":"code","source":["# DQN의 핵심 모델로, 현재 상태를 입력받아 각 행동에 대한 Q값 출력\n","class Qnet(nn.Module):\n","    def __init__(self):\n","        super(Qnet, self).__init__()\n","        self.fc1 = nn.Linear(4, 128)    # 입력 - 상태 공간 크기(4)\n","        self.fc2 = nn.Linear(128, 128)  # 히든\n","        self.fc3 = nn.Linear(128, 2)    # 출력 - 행동 공간 크기(2)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))   # 첫번째, 두번째 층에서 ReLU 활성 함수 적용\n","        x = self.fc3(x)   # 최종 출력값 - 행동별 Q값\n","        return x\n","\n","    def sample_action(self, obs, epsilon):\n","        out = self.forward(obs)   # Q값 계산\n","        coin = random.random()    # 0과 1사이 난수 생성\n","        if coin < epsilon:\n","            return random.randint(0,1)    # 설정한 epsilon보다 난수가 작다면 랜덤 행동 선택\n","        else :\n","            return out.argmax().item()    # 아니라면 최대 Q값을 가진 행동 선택"],"metadata":{"id":"d2tUWrhNbCtZ","executionInfo":{"status":"ok","timestamp":1732604396736,"user_tz":-540,"elapsed":5,"user":{"displayName":"맹의현","userId":"03582999303334923096"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## 학습 함수 정의"],"metadata":{"id":"kZAmt266cARR"}},{"cell_type":"code","source":["# 위에서 정의한 DQN 학습\n","# 경험을 샘플링하여 Q-Network와 Target Q-Network의 차이를 줄이는 것이 목적\n","def train(q, q_target, memory, optimizer):\n","    for i in range(10):\n","        s, a, r, s_prime, done_mask = memory.sample(batch_size)   # 경험을 batch_size만큼 샘플링\n","\n","        # 데이터를 GPU로 이동\n","        s = s.to(device)\n","        a = a.to(device)\n","        r = r.to(device)\n","        s_prime = s_prime.to(device)\n","        done_mask = done_mask.to(device)\n","\n","        q_out = q(s)    # Q값 계산\n","        q_a = q_out.gather(1, a)    # 선택한 행동에 대한 Q값 저장\n","        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)    # 다음 상태 s_prime에 대해 가장 큰 Q값 계산\n","        target = r + gamma * max_q_prime * done_mask    # Bellman Equation\n","        loss = F.smooth_l1_loss(q_a, target)    # Loss 함수로 Huber Loss 사용\n","\n","        optimizer.zero_grad()   # 이전 단계의 optimizer 가중치 초기화\n","        loss.backward()   # 역전파를 이용해 가중치 계산\n","        optimizer.step()    # 가중치 업데이트"],"metadata":{"id":"dmyGVGnP56X6","executionInfo":{"status":"ok","timestamp":1732604396737,"user_tz":-540,"elapsed":6,"user":{"displayName":"맹의현","userId":"03582999303334923096"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## 메인 함수 정의 및 실행"],"metadata":{"id":"NEm0mAWmcG52"}},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcCDfcgrXCM4","executionInfo":{"status":"ok","timestamp":1732604934324,"user_tz":-540,"elapsed":537592,"user":{"displayName":"맹의현","userId":"03582999303334923096"}},"outputId":"1b60e609-61a3-421d-efbc-86df654cd81d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"output_type":"stream","name":"stdout","text":["n_episode :50, score : 13.8, n_buffer : 688, eps : 7.8%\n","n_episode :100, score : 12.8, n_buffer : 1327, eps : 7.5%\n","n_episode :150, score : 13.0, n_buffer : 1978, eps : 7.3%\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-26023cc15c78>:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n","  return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n"]},{"output_type":"stream","name":"stdout","text":["n_episode :200, score : 17.1, n_buffer : 2834, eps : 7.0%\n","n_episode :250, score : 20.8, n_buffer : 3874, eps : 6.8%\n","n_episode :300, score : 23.8, n_buffer : 5062, eps : 6.5%\n","n_episode :350, score : 20.3, n_buffer : 6075, eps : 6.2%\n","n_episode :400, score : 39.9, n_buffer : 8068, eps : 6.0%\n","n_episode :450, score : 46.4, n_buffer : 10388, eps : 5.8%\n","n_episode :500, score : 47.9, n_buffer : 12783, eps : 5.5%\n","n_episode :550, score : 97.5, n_buffer : 17660, eps : 5.3%\n","n_episode :600, score : 123.3, n_buffer : 23825, eps : 5.0%\n","n_episode :650, score : 137.8, n_buffer : 30714, eps : 4.8%\n","n_episode :700, score : 161.0, n_buffer : 38766, eps : 4.5%\n","n_episode :750, score : 148.4, n_buffer : 46186, eps : 4.2%\n","n_episode :800, score : 173.3, n_buffer : 50000, eps : 4.0%\n","n_episode :850, score : 187.9, n_buffer : 50000, eps : 3.8%\n","n_episode :900, score : 183.0, n_buffer : 50000, eps : 3.5%\n","n_episode :950, score : 176.7, n_buffer : 50000, eps : 3.2%\n","n_episode :1000, score : 162.4, n_buffer : 50000, eps : 3.0%\n","n_episode :1050, score : 170.7, n_buffer : 50000, eps : 2.8%\n","n_episode :1100, score : 187.3, n_buffer : 50000, eps : 2.5%\n","n_episode :1150, score : 190.4, n_buffer : 50000, eps : 2.2%\n","n_episode :1200, score : 181.6, n_buffer : 50000, eps : 2.0%\n","n_episode :1250, score : 186.6, n_buffer : 50000, eps : 1.8%\n","n_episode :1300, score : 162.3, n_buffer : 50000, eps : 1.5%\n","n_episode :1350, score : 121.5, n_buffer : 50000, eps : 1.2%\n","n_episode :1400, score : 118.2, n_buffer : 50000, eps : 1.0%\n","n_episode :1450, score : 65.2, n_buffer : 50000, eps : 0.8%\n","n_episode :1500, score : 156.3, n_buffer : 50000, eps : 0.5%\n","n_episode :1550, score : 221.9, n_buffer : 50000, eps : 0.5%\n","n_episode :1600, score : 231.2, n_buffer : 50000, eps : 0.5%\n","n_episode :1650, score : 241.9, n_buffer : 50000, eps : 0.5%\n","n_episode :1700, score : 258.7, n_buffer : 50000, eps : 0.5%\n","n_episode :1750, score : 206.8, n_buffer : 50000, eps : 0.5%\n","n_episode :1800, score : 250.2, n_buffer : 50000, eps : 0.5%\n","n_episode :1850, score : 243.3, n_buffer : 50000, eps : 0.5%\n","n_episode :1900, score : 215.0, n_buffer : 50000, eps : 0.5%\n","n_episode :1950, score : 243.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2000, score : 227.9, n_buffer : 50000, eps : 0.5%\n","n_episode :2050, score : 177.4, n_buffer : 50000, eps : 0.5%\n","n_episode :2100, score : 172.4, n_buffer : 50000, eps : 0.5%\n","n_episode :2150, score : 222.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2200, score : 193.1, n_buffer : 50000, eps : 0.5%\n","n_episode :2250, score : 197.6, n_buffer : 50000, eps : 0.5%\n","n_episode :2300, score : 201.5, n_buffer : 50000, eps : 0.5%\n","n_episode :2350, score : 178.3, n_buffer : 50000, eps : 0.5%\n","n_episode :2400, score : 160.3, n_buffer : 50000, eps : 0.5%\n","n_episode :2450, score : 129.6, n_buffer : 50000, eps : 0.5%\n","n_episode :2500, score : 115.2, n_buffer : 50000, eps : 0.5%\n","n_episode :2550, score : 164.3, n_buffer : 50000, eps : 0.5%\n","n_episode :2600, score : 198.1, n_buffer : 50000, eps : 0.5%\n","n_episode :2650, score : 178.9, n_buffer : 50000, eps : 0.5%\n","n_episode :2700, score : 205.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2750, score : 203.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2800, score : 207.1, n_buffer : 50000, eps : 0.5%\n","n_episode :2850, score : 228.3, n_buffer : 50000, eps : 0.5%\n","n_episode :2900, score : 237.6, n_buffer : 50000, eps : 0.5%\n","n_episode :2950, score : 188.1, n_buffer : 50000, eps : 0.5%\n","n_episode :3000, score : 203.4, n_buffer : 50000, eps : 0.5%\n","n_episode :3050, score : 252.2, n_buffer : 50000, eps : 0.5%\n","n_episode :3100, score : 217.1, n_buffer : 50000, eps : 0.5%\n","n_episode :3150, score : 208.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3200, score : 276.5, n_buffer : 50000, eps : 0.5%\n","n_episode :3250, score : 223.5, n_buffer : 50000, eps : 0.5%\n","n_episode :3300, score : 312.2, n_buffer : 50000, eps : 0.5%\n","n_episode :3350, score : 253.5, n_buffer : 50000, eps : 0.5%\n","n_episode :3400, score : 281.5, n_buffer : 50000, eps : 0.5%\n","n_episode :3450, score : 274.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3500, score : 278.2, n_buffer : 50000, eps : 0.5%\n","n_episode :3550, score : 278.2, n_buffer : 50000, eps : 0.5%\n","n_episode :3600, score : 327.1, n_buffer : 50000, eps : 0.5%\n","n_episode :3650, score : 358.7, n_buffer : 50000, eps : 0.5%\n","n_episode :3700, score : 367.5, n_buffer : 50000, eps : 0.5%\n","n_episode :3750, score : 258.8, n_buffer : 50000, eps : 0.5%\n","n_episode :3800, score : 160.2, n_buffer : 50000, eps : 0.5%\n","n_episode :3850, score : 117.8, n_buffer : 50000, eps : 0.5%\n","n_episode :3900, score : 102.7, n_buffer : 50000, eps : 0.5%\n","n_episode :3950, score : 111.7, n_buffer : 50000, eps : 0.5%\n","n_episode :4000, score : 137.0, n_buffer : 50000, eps : 0.5%\n","n_episode :4050, score : 185.4, n_buffer : 50000, eps : 0.5%\n","n_episode :4100, score : 278.9, n_buffer : 50000, eps : 0.5%\n","n_episode :4150, score : 354.6, n_buffer : 50000, eps : 0.5%\n","n_episode :4200, score : 374.3, n_buffer : 50000, eps : 0.5%\n","n_episode :4250, score : 381.4, n_buffer : 50000, eps : 0.5%\n","n_episode :4300, score : 387.4, n_buffer : 50000, eps : 0.5%\n","n_episode :4350, score : 267.8, n_buffer : 50000, eps : 0.5%\n","n_episode :4400, score : 215.7, n_buffer : 50000, eps : 0.5%\n","n_episode :4450, score : 325.1, n_buffer : 50000, eps : 0.5%\n","n_episode :4500, score : 345.4, n_buffer : 50000, eps : 0.5%\n","n_episode :4550, score : 327.0, n_buffer : 50000, eps : 0.5%\n","n_episode :4600, score : 303.6, n_buffer : 50000, eps : 0.5%\n","n_episode :4650, score : 310.7, n_buffer : 50000, eps : 0.5%\n","n_episode :4700, score : 396.0, n_buffer : 50000, eps : 0.5%\n","n_episode :4750, score : 424.9, n_buffer : 50000, eps : 0.5%\n","n_episode :4800, score : 439.0, n_buffer : 50000, eps : 0.5%\n","n_episode :4850, score : 418.7, n_buffer : 50000, eps : 0.5%\n","n_episode :4900, score : 396.6, n_buffer : 50000, eps : 0.5%\n","n_episode :4950, score : 452.2, n_buffer : 50000, eps : 0.5%\n"]}],"source":["## T4 - 8분\n","def main():\n","    env = gym.make('CartPole-v1')   # 환경 생성\n","    q = Qnet().to(device)    # 새로운 Q-Network 생성\n","    q_target = Qnet().to(device)   # Target Q-Network 생성\n","    q_target.load_state_dict(q.state_dict())    # 가중치 초기화\n","    memory = ReplayBuffer()   # ReplayBuffer 초기화\n","\n","    print_interval = 50   # print 할 에피소드 시점 설정\n","    score = 0.0   # 점수 초기화\n","    optimizer = optim.Adam(q.parameters(), lr=learning_rate)    # Adam optimizer 설정\n","\n","    for n_epi in range(5000):    # 에피소드 반복 횟수(10000->5000)\n","        epsilon = max(0.005, 0.08 - 0.01*(n_epi/200))  # Linear annealing from 8% to 1%\n","        s = env.reset()   # gym의 새로운 버전에서 env.reset()의 반환값이 변경됨\n","        s = torch.tensor(s, dtype=torch.float).to(device)  # 상태를 GPU로 이동\n","        done = False    # 종료조건\n","\n","        while not done:\n","            a = q.sample_action(s, epsilon)  # GPU 텐서를 사용하여 행동 샘플링\n","            s_prime, r, done, info = env.step(a)    # gym의 새로운 버전에서 반환값이 4개로 변경됨\n","            s_prime = torch.tensor(s_prime, dtype=torch.float).to(device)  # 다음 상태를 GPU로 이동\n","            done_mask = 0.0 if done else 1.0    # 선택한 행동을 수행함에 따라 얻은 결과\n","            memory.put((s.cpu().numpy(), a, r / 100.0, s_prime.cpu().numpy(), done_mask))    # 경험 저장\n","            s = s_prime   # 다음 상태로 이동\n","\n","            score += r    # 얻은 보상을 점수에 추가\n","            if done:\n","                break\n","\n","        ## 일정 크기(2000) 이상 ReplayBuffer가 채워졌을 경우 학습 수행\n","        if memory.size()>2000:\n","            train(q, q_target, memory, optimizer)\n","\n","        ## 학습 결과 출력\n","        if n_epi%print_interval==0 and n_epi!=0:\n","            q_target.load_state_dict(q.state_dict())    # Target Q-Network 가중치 업데이트\n","            print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n","                                                            n_epi, score/print_interval, memory.size(), epsilon*100))\n","            score = 0.0   # 점수 초기화\n","    env.close()   # 환경 종료\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"markdown","source":["# (3)"],"metadata":{"id":"anygK19WCRgU"}},{"cell_type":"code","source":["## 연속적인 transition을 입력\n","class SequentialReplayBuffer():\n","    def __init__(self):\n","        self.buffer = collections.deque(maxlen=buffer_limit)    # 경험 저장 공간 설정\n","\n","    def put(self, transition):\n","        self.buffer.append(transition)    # 새로운 경험인 transition을 저장 공간에 추가\n","\n","    def sample(self, n):\n","        if len(self.buffer) < n:    # 데이터 부족 시 예외 처리\n","            raise ValueError(\"Buffer does not contain enough transitions.\")\n","\n","        # 연속된 transition 가져오기\n","        idx = random.randint(0, len(self.buffer) - n)  # 시작 인덱스 설정\n","        mini_batch = list(itertools.islice(self.buffer, idx, idx + n))\n","\n","        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n","\n","        for transition in mini_batch:\n","            s, a, r, s_prime, done_mask = transition\n","            s_lst.append(s)   # 현재 상태(state)\n","            a_lst.append([a])   # 행동(action)\n","            r_lst.append([r])   # 보상(reward)\n","            s_prime_lst.append(s_prime)   # 다음 상태\n","            done_mask_lst.append([done_mask])   # 종료 여부\n","\n","        # 텐서 형태로 반환 수행\n","        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n","               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n","               torch.tensor(done_mask_lst)\n","\n","    def size(self):\n","        return len(self.buffer)   # 저장 공간의 길이 반환"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4SkRPHek5vVU","executionInfo":{"status":"ok","timestamp":1732604934324,"user_tz":-540,"elapsed":6,"user":{"displayName":"맹의현","userId":"03582999303334923096"}},"outputId":"eb952db0-da33-4505-e2ff-acfeccf1ebf5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["# T4 - 10분\n","def main_v2():\n","    env = gym.make('CartPole-v1')   # 환경 생성\n","    q = Qnet().to(device)    # 새로운 Q-Network 생성\n","    q_target = Qnet().to(device)   # Target Q-Network 생성\n","    q_target.load_state_dict(q.state_dict())    # 가중치 초기화\n","    memory = SequentialReplayBuffer()   # ReplayBuffer -> SequentialReplayBuffer 변경\n","\n","    print_interval = 50   # print 할 에피소드 시점 설정\n","    score = 0.0   # 점수 초기화\n","    optimizer = optim.Adam(q.parameters(), lr=learning_rate)    # Adam optimizer 설정\n","\n","    for n_epi in range(5000):    # 에피소드 반복 횟수(10000->5000)\n","        epsilon = max(0.005, 0.08 - 0.01*(n_epi/200))  # Linear annealing from 8% to 1%\n","        s = env.reset()   # gym의 새로운 버전에서 env.reset()의 반환값이 변경됨\n","        s = torch.tensor(s, dtype=torch.float).to(device)  # 상태를 GPU로 이동\n","        done = False    # 종료조건\n","\n","        while not done:\n","            a = q.sample_action(s, epsilon)  # GPU 텐서를 사용하여 행동 샘플링\n","            s_prime, r, done, info = env.step(a)    # gym의 새로운 버전에서 반환값이 4개로 변경됨\n","            s_prime = torch.tensor(s_prime, dtype=torch.float).to(device)  # 다음 상태를 GPU로 이동\n","            done_mask = 0.0 if done else 1.0    # 선택한 행동을 수행함에 따라 얻은 결과\n","            memory.put((s.cpu().numpy(), a, r / 100.0, s_prime.cpu().numpy(), done_mask))    # 경험 저장\n","            s = s_prime   # 다음 상태로 이동\n","\n","            score += r    # 얻은 보상을 점수에 추가\n","            if done:\n","                break\n","\n","        ## 일정 크기(2000) 이상 ReplayBuffer가 채워졌을 경우 학습 수행\n","        if memory.size()>2000:\n","            train(q, q_target, memory, optimizer)\n","\n","        ## 학습 결과 출력\n","        if n_epi%print_interval==0 and n_epi!=0:\n","            q_target.load_state_dict(q.state_dict())    # Target Q-Network 가중치 업데이트\n","            print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n","                                                            n_epi, score/print_interval, memory.size(), epsilon*100))\n","            score = 0.0   # 점수 초기화\n","    env.close()   # 환경 종료\n","\n","if __name__ == '__main__':\n","    main_v2()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWfV8sopEoQd","executionInfo":{"status":"ok","timestamp":1732605570113,"user_tz":-540,"elapsed":635794,"user":{"displayName":"맹의현","userId":"03582999303334923096"}},"outputId":"9a2d54f7-56bc-47c8-bb14-a23278428343"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["n_episode :50, score : 9.9, n_buffer : 494, eps : 7.8%\n","n_episode :100, score : 9.8, n_buffer : 984, eps : 7.5%\n","n_episode :150, score : 9.5, n_buffer : 1457, eps : 7.3%\n","n_episode :200, score : 10.0, n_buffer : 1956, eps : 7.0%\n","n_episode :250, score : 23.0, n_buffer : 3108, eps : 6.8%\n","n_episode :300, score : 35.5, n_buffer : 4885, eps : 6.5%\n","n_episode :350, score : 90.3, n_buffer : 9399, eps : 6.2%\n","n_episode :400, score : 185.9, n_buffer : 18696, eps : 6.0%\n","n_episode :450, score : 192.7, n_buffer : 28330, eps : 5.8%\n","n_episode :500, score : 148.3, n_buffer : 35744, eps : 5.5%\n","n_episode :550, score : 303.0, n_buffer : 50000, eps : 5.3%\n","n_episode :600, score : 229.1, n_buffer : 50000, eps : 5.0%\n","n_episode :650, score : 238.3, n_buffer : 50000, eps : 4.8%\n","n_episode :700, score : 269.3, n_buffer : 50000, eps : 4.5%\n","n_episode :750, score : 253.7, n_buffer : 50000, eps : 4.2%\n","n_episode :800, score : 239.7, n_buffer : 50000, eps : 4.0%\n","n_episode :850, score : 278.8, n_buffer : 50000, eps : 3.8%\n","n_episode :900, score : 260.7, n_buffer : 50000, eps : 3.5%\n","n_episode :950, score : 272.7, n_buffer : 50000, eps : 3.2%\n","n_episode :1000, score : 217.6, n_buffer : 50000, eps : 3.0%\n","n_episode :1050, score : 304.4, n_buffer : 50000, eps : 2.8%\n","n_episode :1100, score : 303.9, n_buffer : 50000, eps : 2.5%\n","n_episode :1150, score : 280.8, n_buffer : 50000, eps : 2.2%\n","n_episode :1200, score : 357.0, n_buffer : 50000, eps : 2.0%\n","n_episode :1250, score : 313.1, n_buffer : 50000, eps : 1.8%\n","n_episode :1300, score : 363.8, n_buffer : 50000, eps : 1.5%\n","n_episode :1350, score : 333.6, n_buffer : 50000, eps : 1.2%\n","n_episode :1400, score : 281.6, n_buffer : 50000, eps : 1.0%\n","n_episode :1450, score : 237.0, n_buffer : 50000, eps : 0.8%\n","n_episode :1500, score : 281.4, n_buffer : 50000, eps : 0.5%\n","n_episode :1550, score : 268.5, n_buffer : 50000, eps : 0.5%\n","n_episode :1600, score : 263.8, n_buffer : 50000, eps : 0.5%\n","n_episode :1650, score : 321.4, n_buffer : 50000, eps : 0.5%\n","n_episode :1700, score : 316.8, n_buffer : 50000, eps : 0.5%\n","n_episode :1750, score : 303.4, n_buffer : 50000, eps : 0.5%\n","n_episode :1800, score : 255.2, n_buffer : 50000, eps : 0.5%\n","n_episode :1850, score : 264.7, n_buffer : 50000, eps : 0.5%\n","n_episode :1900, score : 367.9, n_buffer : 50000, eps : 0.5%\n","n_episode :1950, score : 321.6, n_buffer : 50000, eps : 0.5%\n","n_episode :2000, score : 304.8, n_buffer : 50000, eps : 0.5%\n","n_episode :2050, score : 285.4, n_buffer : 50000, eps : 0.5%\n","n_episode :2100, score : 267.2, n_buffer : 50000, eps : 0.5%\n","n_episode :2150, score : 284.9, n_buffer : 50000, eps : 0.5%\n","n_episode :2200, score : 328.6, n_buffer : 50000, eps : 0.5%\n","n_episode :2250, score : 255.6, n_buffer : 50000, eps : 0.5%\n","n_episode :2300, score : 256.7, n_buffer : 50000, eps : 0.5%\n","n_episode :2350, score : 231.6, n_buffer : 50000, eps : 0.5%\n","n_episode :2400, score : 255.1, n_buffer : 50000, eps : 0.5%\n","n_episode :2450, score : 243.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2500, score : 263.3, n_buffer : 50000, eps : 0.5%\n","n_episode :2550, score : 237.1, n_buffer : 50000, eps : 0.5%\n","n_episode :2600, score : 237.5, n_buffer : 50000, eps : 0.5%\n","n_episode :2650, score : 241.6, n_buffer : 50000, eps : 0.5%\n","n_episode :2700, score : 207.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2750, score : 175.3, n_buffer : 50000, eps : 0.5%\n","n_episode :2800, score : 168.8, n_buffer : 50000, eps : 0.5%\n","n_episode :2850, score : 178.7, n_buffer : 50000, eps : 0.5%\n","n_episode :2900, score : 146.2, n_buffer : 50000, eps : 0.5%\n","n_episode :2950, score : 153.3, n_buffer : 50000, eps : 0.5%\n","n_episode :3000, score : 145.2, n_buffer : 50000, eps : 0.5%\n","n_episode :3050, score : 159.4, n_buffer : 50000, eps : 0.5%\n","n_episode :3100, score : 213.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3150, score : 218.3, n_buffer : 50000, eps : 0.5%\n","n_episode :3200, score : 194.2, n_buffer : 50000, eps : 0.5%\n","n_episode :3250, score : 225.5, n_buffer : 50000, eps : 0.5%\n","n_episode :3300, score : 263.4, n_buffer : 50000, eps : 0.5%\n","n_episode :3350, score : 194.2, n_buffer : 50000, eps : 0.5%\n","n_episode :3400, score : 216.4, n_buffer : 50000, eps : 0.5%\n","n_episode :3450, score : 299.9, n_buffer : 50000, eps : 0.5%\n","n_episode :3500, score : 248.1, n_buffer : 50000, eps : 0.5%\n","n_episode :3550, score : 252.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3600, score : 374.1, n_buffer : 50000, eps : 0.5%\n","n_episode :3650, score : 353.9, n_buffer : 50000, eps : 0.5%\n","n_episode :3700, score : 251.3, n_buffer : 50000, eps : 0.5%\n","n_episode :3750, score : 387.1, n_buffer : 50000, eps : 0.5%\n","n_episode :3800, score : 360.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3850, score : 357.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3900, score : 325.6, n_buffer : 50000, eps : 0.5%\n","n_episode :3950, score : 359.5, n_buffer : 50000, eps : 0.5%\n","n_episode :4000, score : 358.8, n_buffer : 50000, eps : 0.5%\n","n_episode :4050, score : 373.6, n_buffer : 50000, eps : 0.5%\n","n_episode :4100, score : 342.1, n_buffer : 50000, eps : 0.5%\n","n_episode :4150, score : 415.1, n_buffer : 50000, eps : 0.5%\n","n_episode :4200, score : 418.0, n_buffer : 50000, eps : 0.5%\n","n_episode :4250, score : 389.1, n_buffer : 50000, eps : 0.5%\n","n_episode :4300, score : 413.4, n_buffer : 50000, eps : 0.5%\n","n_episode :4350, score : 355.8, n_buffer : 50000, eps : 0.5%\n","n_episode :4400, score : 416.1, n_buffer : 50000, eps : 0.5%\n","n_episode :4450, score : 317.6, n_buffer : 50000, eps : 0.5%\n","n_episode :4500, score : 230.0, n_buffer : 50000, eps : 0.5%\n","n_episode :4550, score : 281.8, n_buffer : 50000, eps : 0.5%\n","n_episode :4600, score : 283.9, n_buffer : 50000, eps : 0.5%\n","n_episode :4650, score : 268.8, n_buffer : 50000, eps : 0.5%\n","n_episode :4700, score : 244.4, n_buffer : 50000, eps : 0.5%\n","n_episode :4750, score : 328.5, n_buffer : 50000, eps : 0.5%\n","n_episode :4800, score : 320.4, n_buffer : 50000, eps : 0.5%\n","n_episode :4850, score : 185.2, n_buffer : 50000, eps : 0.5%\n","n_episode :4900, score : 255.8, n_buffer : 50000, eps : 0.5%\n","n_episode :4950, score : 386.1, n_buffer : 50000, eps : 0.5%\n"]}]},{"cell_type":"markdown","source":["### 실험결과<br />\n","\n","큰 차이는 없었지만, 그럼에도 다소 성능이 떨어지고 안정성이 부족해보임을 확인할 수 있음.<br />샘플의 다양성이 부족하고 에폭이 많아지면 과적합 위험성이 존재\n"],"metadata":{"id":"5qtwkQ7IE4yn"}},{"cell_type":"markdown","source":["# (4)"],"metadata":{"id":"YKn64KqxGWvT"}},{"cell_type":"code","source":["def train_v2(q, memory, optimizer):\n","    for i in range(10):\n","        s, a, r, s_prime, done_mask = memory.sample(batch_size)\n","\n","        # 데이터를 GPU로 이동\n","        s = s.to(device)\n","        a = a.to(device)\n","        r = r.to(device)\n","        s_prime = s_prime.to(device)\n","        done_mask = done_mask.to(device)\n","\n","        q_out = q(s)\n","        q_a = q_out.gather(1, a)  # 선택한 행동에 대한 Q값\n","        max_q_prime = q(s_prime).max(1)[0].unsqueeze(1)  # 동일 Q-network로 다음 상태 Q값 계산\n","        target = r + gamma * max_q_prime * done_mask  # Bellman Equation\n","        loss = F.smooth_l1_loss(q_a, target)  # Huber Loss\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()"],"metadata":{"id":"CLkqw0QiGYXX","executionInfo":{"status":"ok","timestamp":1732605570114,"user_tz":-540,"elapsed":4,"user":{"displayName":"맹의현","userId":"03582999303334923096"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# T4 - 7분\n","def main_v3():\n","    env = gym.make('CartPole-v1')   # 환경 생성\n","    q = Qnet().to(device)    # 새로운 Q-Network 생성\n","    memory = ReplayBuffer()   # ReplayBuffer 초기화\n","\n","    print_interval = 50   # print 할 에피소드 시점 설정\n","    score = 0.0   # 점수 초기화\n","    optimizer = optim.Adam(q.parameters(), lr=learning_rate)    # Adam optimizer 설정\n","\n","    for n_epi in range(5000):    # 에피소드 반복 횟수(10000->5000)\n","        epsilon = max(0.005, 0.08 - 0.01*(n_epi/200))  # Linear annealing from 8% to 1%\n","        s = env.reset()   # gym의 새로운 버전에서 env.reset()의 반환값이 변경됨\n","        s = torch.tensor(s, dtype=torch.float).to(device)  # 상태를 GPU로 이동\n","        done = False    # 종료조건\n","\n","        while not done:\n","            a = q.sample_action(s, epsilon)  # GPU 텐서를 사용하여 행동 샘플링\n","            step_result = env.step(a)  # Step API 대응\n","            if len(step_result) == 4:\n","                s_prime, r, done, info = step_result\n","            else:\n","                s_prime, r, terminated, truncated, info = step_result\n","                done = terminated or truncated\n","\n","            s_prime = torch.tensor(s_prime, dtype=torch.float).to(device)  # 다음 상태를 GPU로 이동\n","            done_mask = 0.0 if done else 1.0    # 선택한 행동을 수행함에 따라 얻은 결과\n","            memory.put((s.cpu().numpy(), a, r / 100.0, s_prime.cpu().numpy(), done_mask))    # 경험 저장\n","            s = s_prime   # 다음 상태로 이동\n","\n","            score += r    # 얻은 보상을 점수에 추가\n","            if done:\n","                break\n","\n","        ## 일정 크기(2000) 이상 ReplayBuffer가 채워졌을 경우 학습 수행\n","        if memory.size()>2000:\n","            train_v2(q, memory, optimizer)\n","\n","        ## 학습 결과 출력\n","        if n_epi%print_interval==0 and n_epi!=0:\n","            print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n","                                                            n_epi, score/print_interval, memory.size(), epsilon*100))\n","            score = 0.0   # 점수 초기화\n","    env.close()   # 환경 종료\n","\n","if __name__ == '__main__':\n","    main_v3()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GR2ApAexHneS","executionInfo":{"status":"ok","timestamp":1732606026743,"user_tz":-540,"elapsed":456632,"user":{"displayName":"맹의현","userId":"03582999303334923096"}},"outputId":"a1787b60-2aa2-4b02-af47-4fe472e46c8a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["n_episode :50, score : 10.4, n_buffer : 522, eps : 7.8%\n","n_episode :100, score : 10.7, n_buffer : 1056, eps : 7.5%\n","n_episode :150, score : 10.6, n_buffer : 1588, eps : 7.3%\n","n_episode :200, score : 10.9, n_buffer : 2131, eps : 7.0%\n","n_episode :250, score : 33.9, n_buffer : 3828, eps : 6.8%\n","n_episode :300, score : 50.7, n_buffer : 6365, eps : 6.5%\n","n_episode :350, score : 52.0, n_buffer : 8963, eps : 6.2%\n","n_episode :400, score : 77.5, n_buffer : 12839, eps : 6.0%\n","n_episode :450, score : 83.0, n_buffer : 16989, eps : 5.8%\n","n_episode :500, score : 125.0, n_buffer : 23239, eps : 5.5%\n","n_episode :550, score : 131.5, n_buffer : 29815, eps : 5.3%\n","n_episode :600, score : 142.7, n_buffer : 36951, eps : 5.0%\n","n_episode :650, score : 155.7, n_buffer : 44736, eps : 4.8%\n","n_episode :700, score : 187.5, n_buffer : 50000, eps : 4.5%\n","n_episode :750, score : 198.7, n_buffer : 50000, eps : 4.2%\n","n_episode :800, score : 191.5, n_buffer : 50000, eps : 4.0%\n","n_episode :850, score : 112.7, n_buffer : 50000, eps : 3.8%\n","n_episode :900, score : 103.4, n_buffer : 50000, eps : 3.5%\n","n_episode :950, score : 139.4, n_buffer : 50000, eps : 3.2%\n","n_episode :1000, score : 105.2, n_buffer : 50000, eps : 3.0%\n","n_episode :1050, score : 156.3, n_buffer : 50000, eps : 2.8%\n","n_episode :1100, score : 138.4, n_buffer : 50000, eps : 2.5%\n","n_episode :1150, score : 173.0, n_buffer : 50000, eps : 2.2%\n","n_episode :1200, score : 139.1, n_buffer : 50000, eps : 2.0%\n","n_episode :1250, score : 175.3, n_buffer : 50000, eps : 1.8%\n","n_episode :1300, score : 154.0, n_buffer : 50000, eps : 1.5%\n","n_episode :1350, score : 177.5, n_buffer : 50000, eps : 1.2%\n","n_episode :1400, score : 168.9, n_buffer : 50000, eps : 1.0%\n","n_episode :1450, score : 165.2, n_buffer : 50000, eps : 0.8%\n","n_episode :1500, score : 213.2, n_buffer : 50000, eps : 0.5%\n","n_episode :1550, score : 222.0, n_buffer : 50000, eps : 0.5%\n","n_episode :1600, score : 175.7, n_buffer : 50000, eps : 0.5%\n","n_episode :1650, score : 169.2, n_buffer : 50000, eps : 0.5%\n","n_episode :1700, score : 168.8, n_buffer : 50000, eps : 0.5%\n","n_episode :1750, score : 181.8, n_buffer : 50000, eps : 0.5%\n","n_episode :1800, score : 172.9, n_buffer : 50000, eps : 0.5%\n","n_episode :1850, score : 166.2, n_buffer : 50000, eps : 0.5%\n","n_episode :1900, score : 156.6, n_buffer : 50000, eps : 0.5%\n","n_episode :1950, score : 227.5, n_buffer : 50000, eps : 0.5%\n","n_episode :2000, score : 185.3, n_buffer : 50000, eps : 0.5%\n","n_episode :2050, score : 179.1, n_buffer : 50000, eps : 0.5%\n","n_episode :2100, score : 207.4, n_buffer : 50000, eps : 0.5%\n","n_episode :2150, score : 201.8, n_buffer : 50000, eps : 0.5%\n","n_episode :2200, score : 204.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2250, score : 157.4, n_buffer : 50000, eps : 0.5%\n","n_episode :2300, score : 147.7, n_buffer : 50000, eps : 0.5%\n","n_episode :2350, score : 160.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2400, score : 138.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2450, score : 156.2, n_buffer : 50000, eps : 0.5%\n","n_episode :2500, score : 255.8, n_buffer : 50000, eps : 0.5%\n","n_episode :2550, score : 216.4, n_buffer : 50000, eps : 0.5%\n","n_episode :2600, score : 238.6, n_buffer : 50000, eps : 0.5%\n","n_episode :2650, score : 189.9, n_buffer : 50000, eps : 0.5%\n","n_episode :2700, score : 225.6, n_buffer : 50000, eps : 0.5%\n","n_episode :2750, score : 195.1, n_buffer : 50000, eps : 0.5%\n","n_episode :2800, score : 166.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2850, score : 174.1, n_buffer : 50000, eps : 0.5%\n","n_episode :2900, score : 163.5, n_buffer : 50000, eps : 0.5%\n","n_episode :2950, score : 192.2, n_buffer : 50000, eps : 0.5%\n","n_episode :3000, score : 263.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3050, score : 193.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3100, score : 163.1, n_buffer : 50000, eps : 0.5%\n","n_episode :3150, score : 169.3, n_buffer : 50000, eps : 0.5%\n","n_episode :3200, score : 206.9, n_buffer : 50000, eps : 0.5%\n","n_episode :3250, score : 227.2, n_buffer : 50000, eps : 0.5%\n","n_episode :3300, score : 185.6, n_buffer : 50000, eps : 0.5%\n","n_episode :3350, score : 165.4, n_buffer : 50000, eps : 0.5%\n","n_episode :3400, score : 151.1, n_buffer : 50000, eps : 0.5%\n","n_episode :3450, score : 141.5, n_buffer : 50000, eps : 0.5%\n","n_episode :3500, score : 140.8, n_buffer : 50000, eps : 0.5%\n","n_episode :3550, score : 147.4, n_buffer : 50000, eps : 0.5%\n","n_episode :3600, score : 154.4, n_buffer : 50000, eps : 0.5%\n","n_episode :3650, score : 205.4, n_buffer : 50000, eps : 0.5%\n","n_episode :3700, score : 200.3, n_buffer : 50000, eps : 0.5%\n","n_episode :3750, score : 202.5, n_buffer : 50000, eps : 0.5%\n","n_episode :3800, score : 265.6, n_buffer : 50000, eps : 0.5%\n","n_episode :3850, score : 170.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3900, score : 160.8, n_buffer : 50000, eps : 0.5%\n","n_episode :3950, score : 146.5, n_buffer : 50000, eps : 0.5%\n","n_episode :4000, score : 129.6, n_buffer : 50000, eps : 0.5%\n","n_episode :4050, score : 170.3, n_buffer : 50000, eps : 0.5%\n","n_episode :4100, score : 170.2, n_buffer : 50000, eps : 0.5%\n","n_episode :4150, score : 216.2, n_buffer : 50000, eps : 0.5%\n","n_episode :4200, score : 228.5, n_buffer : 50000, eps : 0.5%\n","n_episode :4250, score : 190.5, n_buffer : 50000, eps : 0.5%\n","n_episode :4300, score : 198.4, n_buffer : 50000, eps : 0.5%\n","n_episode :4350, score : 171.8, n_buffer : 50000, eps : 0.5%\n","n_episode :4400, score : 162.6, n_buffer : 50000, eps : 0.5%\n","n_episode :4450, score : 156.1, n_buffer : 50000, eps : 0.5%\n","n_episode :4500, score : 151.7, n_buffer : 50000, eps : 0.5%\n","n_episode :4550, score : 189.0, n_buffer : 50000, eps : 0.5%\n","n_episode :4600, score : 195.3, n_buffer : 50000, eps : 0.5%\n","n_episode :4650, score : 228.6, n_buffer : 50000, eps : 0.5%\n","n_episode :4700, score : 264.2, n_buffer : 50000, eps : 0.5%\n","n_episode :4750, score : 251.9, n_buffer : 50000, eps : 0.5%\n","n_episode :4800, score : 194.2, n_buffer : 50000, eps : 0.5%\n","n_episode :4850, score : 189.1, n_buffer : 50000, eps : 0.5%\n","n_episode :4900, score : 178.2, n_buffer : 50000, eps : 0.5%\n","n_episode :4950, score : 174.5, n_buffer : 50000, eps : 0.5%\n"]}]},{"cell_type":"markdown","source":["### 실험결과<br />\n","\n","단일 네트워크를 사용해 실행시간이 줄어들었지만(10분->7분), 성능이 비교적 매우 떨어지는 것을 확인할 수 있음."],"metadata":{"id":"sGuy8CN6hsBu"}},{"cell_type":"markdown","source":["# (5)"],"metadata":{"id":"O1LN23eMI6jc"}},{"cell_type":"code","source":["# T4 - 7분\n","def main_v4():\n","    env = gym.make('CartPole-v1')   # 환경 생성\n","    q = Qnet().to(device)    # 새로운 Q-Network 생성\n","    memory = SequentialReplayBuffer()   # ReplayBuffer -> SequentialReplayBuffer 변경\n","\n","    print_interval = 50   # print 할 에피소드 시점 설정\n","    score = 0.0   # 점수 초기화\n","    optimizer = optim.Adam(q.parameters(), lr=learning_rate)    # Adam optimizer 설정\n","\n","    for n_epi in range(5000):    # 에피소드 반복 횟수(10000->5000)\n","        epsilon = max(0.005, 0.08 - 0.01*(n_epi/200))  # Linear annealing from 8% to 1%\n","        s = env.reset()   # gym의 새로운 버전에서 env.reset()의 반환값이 변경됨\n","        s = torch.tensor(s, dtype=torch.float).to(device)  # 상태를 GPU로 이동\n","        done = False    # 종료조건\n","\n","        while not done:\n","            a = q.sample_action(s, epsilon)  # GPU 텐서를 사용하여 행동 샘플링\n","            step_result = env.step(a)  # Step API 대응\n","            if len(step_result) == 4:\n","                s_prime, r, done, info = step_result\n","            else:\n","                s_prime, r, terminated, truncated, info = step_result\n","                done = terminated or truncated\n","\n","            s_prime = torch.tensor(s_prime, dtype=torch.float).to(device)  # 다음 상태를 GPU로 이동\n","            done_mask = 0.0 if done else 1.0    # 선택한 행동을 수행함에 따라 얻은 결과\n","            memory.put((s.cpu().numpy(), a, r / 100.0, s_prime.cpu().numpy(), done_mask))    # 경험 저장\n","            s = s_prime   # 다음 상태로 이동\n","\n","            score += r    # 얻은 보상을 점수에 추가\n","            if done:\n","                break\n","\n","        ## 일정 크기(2000) 이상 ReplayBuffer가 채워졌을 경우 학습 수행\n","        if memory.size()>2000:\n","            train_v2(q, memory, optimizer)\n","\n","        ## 학습 결과 출력\n","        if n_epi%print_interval==0 and n_epi!=0:\n","            print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n","                                                            n_epi, score/print_interval, memory.size(), epsilon*100))\n","            score = 0.0   # 점수 초기화\n","    env.close()   # 환경 종료\n","\n","if __name__ == '__main__':\n","    main_v4()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-CXwDoFHI76M","executionInfo":{"status":"ok","timestamp":1732606477154,"user_tz":-540,"elapsed":450423,"user":{"displayName":"맹의현","userId":"03582999303334923096"}},"outputId":"0698ec0c-3606-49a0-bf61-0a1eed38dc50"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["n_episode :50, score : 10.0, n_buffer : 502, eps : 7.8%\n","n_episode :100, score : 9.6, n_buffer : 981, eps : 7.5%\n","n_episode :150, score : 9.7, n_buffer : 1468, eps : 7.3%\n","n_episode :200, score : 9.8, n_buffer : 1959, eps : 7.0%\n","n_episode :250, score : 19.2, n_buffer : 2917, eps : 6.8%\n","n_episode :300, score : 49.6, n_buffer : 5398, eps : 6.5%\n","n_episode :350, score : 52.6, n_buffer : 8028, eps : 6.2%\n","n_episode :400, score : 62.0, n_buffer : 11127, eps : 6.0%\n","n_episode :450, score : 67.3, n_buffer : 14492, eps : 5.8%\n","n_episode :500, score : 69.6, n_buffer : 17970, eps : 5.5%\n","n_episode :550, score : 68.6, n_buffer : 21398, eps : 5.3%\n","n_episode :600, score : 94.0, n_buffer : 26098, eps : 5.0%\n","n_episode :650, score : 130.7, n_buffer : 32634, eps : 4.8%\n","n_episode :700, score : 119.3, n_buffer : 38598, eps : 4.5%\n","n_episode :750, score : 145.1, n_buffer : 45853, eps : 4.2%\n","n_episode :800, score : 140.4, n_buffer : 50000, eps : 4.0%\n","n_episode :850, score : 130.3, n_buffer : 50000, eps : 3.8%\n","n_episode :900, score : 127.4, n_buffer : 50000, eps : 3.5%\n","n_episode :950, score : 133.9, n_buffer : 50000, eps : 3.2%\n","n_episode :1000, score : 131.1, n_buffer : 50000, eps : 3.0%\n","n_episode :1050, score : 138.7, n_buffer : 50000, eps : 2.8%\n","n_episode :1100, score : 151.7, n_buffer : 50000, eps : 2.5%\n","n_episode :1150, score : 165.3, n_buffer : 50000, eps : 2.2%\n","n_episode :1200, score : 155.3, n_buffer : 50000, eps : 2.0%\n","n_episode :1250, score : 173.4, n_buffer : 50000, eps : 1.8%\n","n_episode :1300, score : 216.8, n_buffer : 50000, eps : 1.5%\n","n_episode :1350, score : 162.6, n_buffer : 50000, eps : 1.2%\n","n_episode :1400, score : 150.5, n_buffer : 50000, eps : 1.0%\n","n_episode :1450, score : 206.1, n_buffer : 50000, eps : 0.8%\n","n_episode :1500, score : 196.1, n_buffer : 50000, eps : 0.5%\n","n_episode :1550, score : 237.5, n_buffer : 50000, eps : 0.5%\n","n_episode :1600, score : 232.2, n_buffer : 50000, eps : 0.5%\n","n_episode :1650, score : 183.8, n_buffer : 50000, eps : 0.5%\n","n_episode :1700, score : 200.6, n_buffer : 50000, eps : 0.5%\n","n_episode :1750, score : 162.8, n_buffer : 50000, eps : 0.5%\n","n_episode :1800, score : 168.7, n_buffer : 50000, eps : 0.5%\n","n_episode :1850, score : 178.7, n_buffer : 50000, eps : 0.5%\n","n_episode :1900, score : 150.8, n_buffer : 50000, eps : 0.5%\n","n_episode :1950, score : 166.8, n_buffer : 50000, eps : 0.5%\n","n_episode :2000, score : 170.7, n_buffer : 50000, eps : 0.5%\n","n_episode :2050, score : 196.1, n_buffer : 50000, eps : 0.5%\n","n_episode :2100, score : 112.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2150, score : 231.4, n_buffer : 50000, eps : 0.5%\n","n_episode :2200, score : 181.8, n_buffer : 50000, eps : 0.5%\n","n_episode :2250, score : 143.4, n_buffer : 50000, eps : 0.5%\n","n_episode :2300, score : 170.4, n_buffer : 50000, eps : 0.5%\n","n_episode :2350, score : 192.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2400, score : 175.7, n_buffer : 50000, eps : 0.5%\n","n_episode :2450, score : 206.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2500, score : 212.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2550, score : 177.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2600, score : 142.9, n_buffer : 50000, eps : 0.5%\n","n_episode :2650, score : 169.9, n_buffer : 50000, eps : 0.5%\n","n_episode :2700, score : 154.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2750, score : 170.0, n_buffer : 50000, eps : 0.5%\n","n_episode :2800, score : 151.6, n_buffer : 50000, eps : 0.5%\n","n_episode :2850, score : 154.7, n_buffer : 50000, eps : 0.5%\n","n_episode :2900, score : 199.7, n_buffer : 50000, eps : 0.5%\n","n_episode :2950, score : 225.7, n_buffer : 50000, eps : 0.5%\n","n_episode :3000, score : 156.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3050, score : 159.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3100, score : 152.5, n_buffer : 50000, eps : 0.5%\n","n_episode :3150, score : 182.4, n_buffer : 50000, eps : 0.5%\n","n_episode :3200, score : 172.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3250, score : 151.8, n_buffer : 50000, eps : 0.5%\n","n_episode :3300, score : 137.4, n_buffer : 50000, eps : 0.5%\n","n_episode :3350, score : 179.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3400, score : 190.5, n_buffer : 50000, eps : 0.5%\n","n_episode :3450, score : 151.3, n_buffer : 50000, eps : 0.5%\n","n_episode :3500, score : 188.4, n_buffer : 50000, eps : 0.5%\n","n_episode :3550, score : 173.0, n_buffer : 50000, eps : 0.5%\n","n_episode :3600, score : 245.9, n_buffer : 50000, eps : 0.5%\n","n_episode :3650, score : 211.3, n_buffer : 50000, eps : 0.5%\n","n_episode :3700, score : 171.9, n_buffer : 50000, eps : 0.5%\n","n_episode :3750, score : 212.6, n_buffer : 50000, eps : 0.5%\n","n_episode :3800, score : 232.5, n_buffer : 50000, eps : 0.5%\n","n_episode :3850, score : 171.8, n_buffer : 50000, eps : 0.5%\n","n_episode :3900, score : 209.2, n_buffer : 50000, eps : 0.5%\n","n_episode :3950, score : 184.2, n_buffer : 50000, eps : 0.5%\n","n_episode :4000, score : 183.9, n_buffer : 50000, eps : 0.5%\n","n_episode :4050, score : 142.2, n_buffer : 50000, eps : 0.5%\n","n_episode :4100, score : 146.5, n_buffer : 50000, eps : 0.5%\n","n_episode :4150, score : 158.1, n_buffer : 50000, eps : 0.5%\n","n_episode :4200, score : 139.9, n_buffer : 50000, eps : 0.5%\n","n_episode :4250, score : 166.9, n_buffer : 50000, eps : 0.5%\n","n_episode :4300, score : 166.3, n_buffer : 50000, eps : 0.5%\n","n_episode :4350, score : 227.8, n_buffer : 50000, eps : 0.5%\n","n_episode :4400, score : 208.0, n_buffer : 50000, eps : 0.5%\n","n_episode :4450, score : 178.7, n_buffer : 50000, eps : 0.5%\n","n_episode :4500, score : 201.9, n_buffer : 50000, eps : 0.5%\n","n_episode :4550, score : 186.0, n_buffer : 50000, eps : 0.5%\n","n_episode :4600, score : 200.1, n_buffer : 50000, eps : 0.5%\n","n_episode :4650, score : 215.2, n_buffer : 50000, eps : 0.5%\n","n_episode :4700, score : 243.3, n_buffer : 50000, eps : 0.5%\n","n_episode :4750, score : 227.3, n_buffer : 50000, eps : 0.5%\n","n_episode :4800, score : 265.7, n_buffer : 50000, eps : 0.5%\n","n_episode :4850, score : 259.7, n_buffer : 50000, eps : 0.5%\n","n_episode :4900, score : 240.9, n_buffer : 50000, eps : 0.5%\n","n_episode :4950, score : 196.6, n_buffer : 50000, eps : 0.5%\n"]}]},{"cell_type":"markdown","source":["### 실험결과<br />\n","\n","naive DQN은 구조가 간단하지만, 기존 DQN 코드보다 성능이 훨씬 떨어지는 것을 알 수 있었음.\n","\n"],"metadata":{"id":"o-zMud4ciIls"}},{"cell_type":"markdown","source":["# (6)"],"metadata":{"id":"4MpF0moDJBZv"}},{"cell_type":"markdown","source":[" <table>\n","  <thead>\n","    <tr>\n","      <th><b>모델/변경 사항</b></th>\n","      <th><b>특징</b></th>\n","      <th><b>결론</b></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td><b>(2) 원래의 DQN</b></td>\n","      <td>안정적인 학습과 빠른 수렴 가능<br />Hyperparameter 튜닝이 필요</td>\n","      <td>가장 균형 잡힌 성능을 제공</td>\n","    </tr>\n","    <tr>\n","      <td><b>(3) 연속 transition 샘플링</b></td>\n","      <td>샘플 다양성 부족, 과적합 가능성</td>\n","      <td>특정 환경에서 유용할 수 있지만, 일반적인 안정성은 떨어짐</td>\n","    </tr>\n","    <tr>\n","      <td><b>(4) Target Network 미사용</b></td>\n","      <td>구조가 간단<br />학습 불안정성 증가, Q-value 발산 가능성</td>\n","      <td>Target Network 없이 안정적인 학습은 어려움</td>\n","    </tr>\n","    <tr>\n","      <td><b>(5) naive DQN</b></td>\n","      <td>위 두 가지 단점 결합: 과적합 및 불안정성</td>\n","      <td>초보적인 실험에서는 유용하나 실제 성능은 가장 낮음</td>\n","    </tr>\n","  </tbody>\n","</table>"],"metadata":{"id":"Y6rVgAqGjgY1"}},{"cell_type":"code","source":[],"metadata":{"id":"3QYW_83uJCtx"},"execution_count":null,"outputs":[]}]}