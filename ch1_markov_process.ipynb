{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Markov Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Markov Process (Weather Model)\n",
    "맑음과 비, 두 가지 상태를 가진 마르코프 프로세스를 시뮬레이션 해 보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather model\n",
    "# state transition probability matrix\n",
    "# index 0: Sunny, index 1: Rainy\n",
    "P = [[0.8, 0.2],\n",
    "     [0.1, 0.9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5506204096605429"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a random number between 0 and 1 from uniform distribution\n",
    "# run this cell many times and observe how number varies\n",
    "np.random.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1],\n",
       " [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling of Markov process (2 states)\n",
    "num_episodes = 7 # 에피소드의 갯수 (예: 7일 동안의 관찰)\n",
    "list_episodes = [] # 에피소드들을 모아놓기 위한 리스트\n",
    "length_episode = 24 # 한 에피소드의 길이 (예: 24시간 동안 날씨 관찰)\n",
    "\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    # start of a new episode\n",
    "    episode = [] # 한 에피소드를 기록하기 위한 리스트\n",
    "    s = 0 # current state\n",
    "    for t in range(length_episode):\n",
    "        prob = np.random.uniform()\n",
    "        s = 0 if prob <= P[s][0] else 1\n",
    "        episode.append(s)\n",
    "    \n",
    "    list_episodes.append(episode)\n",
    "\n",
    "list_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플링 결과는 매 번 실행할 때마다 바뀐다(random process).  \n",
    "행렬 P의 대각선(diagonal) 값들이 각각 0.8, 0.9로 크다는 것은 현재 상태(0 또는 1)가 한동안 계속 유지될 가능성이 높다는 뜻.  \n",
    "P의 원소들을 임의로 바꿔 가면서 실험해 보자(단, 각 row 방향의 합은 항상 1이 되어야 한다).  \n",
    "많은 반복을 거듭했을 때, 평균적으로 각 상태에 머무는 비율(확률)은 얼마가 되는가?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Markov Process (Employee Model)\n",
    "집, 커피, 대화, 컴퓨터라는 4개의 상태를 가진 회사원 모델을 마르코프 프로세스로 시뮬레이션 해 보자.  \n",
    "임의의 n개의 상태를 가진 Markov process를 시뮬레이션하기 위해  \n",
    "상태변화확률이 n x n 행렬로 주어질 때, 상태 변화를 결정하는 함수를 만들어 보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6 1.  1.  1. ]\n",
      " [0.  0.1 0.8 1. ]\n",
      " [0.  0.2 0.7 1. ]\n",
      " [0.2 0.4 0.5 1. ]]\n"
     ]
    }
   ],
   "source": [
    "# employee model\n",
    "# state transition matrix\n",
    "# index: 0: home, 1: coffee, 2: chat, 3: computer\n",
    "P = [[0.6, 0.4, 0.0, 0.0],\n",
    "     [0.0, 0.1, 0.7, 0.2],\n",
    "     [0.0, 0.2, 0.5, 0.3],\n",
    "     [0.2, 0.2, 0.1, 0.5]]\n",
    "\n",
    "csP = np.cumsum(P, axis=1)\n",
    "print(csP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "zero_vec = np.zeros((4, 1))\n",
    "print(zero_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.6 1.  1.  1. ]\n",
      " [0.  0.  0.1 0.8 1. ]\n",
      " [0.  0.  0.2 0.7 1. ]\n",
      " [0.  0.2 0.4 0.5 1. ]]\n"
     ]
    }
   ],
   "source": [
    "csP = np.concatenate((zero_vec, csP), axis=1)\n",
    "print(csP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(s, P):\n",
    "    '''\n",
    "    현재 상태 s와 state transition matrix P가 주어질 때,\n",
    "    다음 상태 s를 반환하는 함수\n",
    "\n",
    "    input:\n",
    "    s: 0에서 n-1까지의 값을 가질 수 있는 정수 (상태)\n",
    "    P: n x n 행렬 (state transtion matrix)\n",
    "\n",
    "    output:\n",
    "    next_s: 확률에 의해 결정된 다음 상태\n",
    "    '''\n",
    "\n",
    "    n = len(P) # P 행렬의 행의 갯수 (= 상태의 갯수)\n",
    "\n",
    "    # cumulative sum of the state transition matrix \n",
    "    csP = np.cumsum(P, axis=1) # sum along rows\n",
    "    zero_vec = np.zeros((n, 1)) # a column vector (nx1 matrix) with zero elements\n",
    "    csP = np.concatenate((zero_vec, csP), axis=1) # concatenate two matrices\n",
    "\n",
    "    prob = np.random.uniform()\n",
    "    for k in range(n):\n",
    "        if (prob >= csP[s][k]) and (prob < csP[s][k+1]):\n",
    "            next_s = k\n",
    "            break\n",
    "\n",
    "    return next_s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 1, 3, 3, 2, 3, 3, 2, 1, 2, 3, 1, 2, 1, 2, 2, 1, 3, 3, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 2, 3, 0, 1, 3, 3, 3, 1, 1, 2, 2, 2, 3, 0, 1, 2, 1, 3, 2],\n",
       " [0, 0, 0, 0, 1, 2, 2, 3, 3, 3, 2, 2, 3, 0, 1, 1, 3, 3, 3, 2, 1, 3, 0, 1],\n",
       " [0, 0, 1, 2, 2, 2, 2, 2, 3, 3, 0, 0, 0, 0, 1, 2, 2, 1, 2, 2, 1, 2, 2, 3],\n",
       " [0, 0, 1, 2, 3, 0, 0, 0, 0, 1, 3, 2, 3, 3, 3, 3, 2, 3, 1, 2, 1, 3, 2, 2],\n",
       " [0, 1, 2, 3, 3, 1, 2, 1, 2, 3, 0, 1, 2, 2, 2, 3, 3, 0, 1, 3, 3, 3, 1, 1],\n",
       " [0, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling of Markov process (4 states)\n",
    "num_episodes = 7\n",
    "list_episodes = []\n",
    "length_episode = 24\n",
    "\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    # start of a new episode\n",
    "    episode = []\n",
    "    s = 0 # current state\n",
    "    for t in range(length_episode):\n",
    "        s = next_state(s, P) # 다음 상태를 다시 s로 업데이트\n",
    "        episode.append(s)\n",
    "    \n",
    "    list_episodes.append(episode)\n",
    "\n",
    "list_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지의 예시에서는, 어떤 상태도 최종적이지 않고 다시 다른 상태로 바뀔 수 있었다.  \n",
    "따라서 이론적으로는 마르코프 프로세스가 무한히 반복될 수 있으므로, 하나의 에피소드 길이를 고정해서 시뮬레이션 했다.  \n",
    "만약 어떤 상태가 자기 자신으로 돌아오는 확률이 1이면 나머지로 변할 확률이 자연히 0이 되고, 이 경우 이 상태에 도달하면 다른 상태로 바뀔 수 없다.  \n",
    "이러한 상태를 최종 상태(terminal state)라 하고, 한 에피소드는 최종 상태에 도달하면 유한한 횟수 안에 끝나게 된다.  \n",
    "\n",
    "이제, 위의 회사원 예제에서 시작 상태가 3(컴퓨터)이고 상태 0(집)에 도달하게 되면, 한 에피소드 (예: 하루)의 끝이 되게 만들어 보자.  \n",
    "이를 위해 상태변환 행렬 P의 값을 수정한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employee model - 2\n",
    "# new state transition matrix\n",
    "# index: 0: home, 1: coffee, 2: chat, 3: computer\n",
    "# changed the first row of P\n",
    "P = [[1.0, 0.0, 0.0, 0.0],\n",
    "     [0.0, 0.1, 0.7, 0.2],\n",
    "     [0.0, 0.2, 0.5, 0.3],\n",
    "     [0.2, 0.2, 0.1, 0.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마르코프 프로세스를 시뮬레이션 할 때, 에피소드 길이의 제한을 아주 크게 두고 최종 상태(0)에 도달하게 하면 한 에피소드가 끝나도록 코드를 수정하자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 0],\n",
       " [3, 0],\n",
       " [1, 1, 3, 3, 1, 3, 0],\n",
       " [1, 2, 2, 2, 2, 1, 3, 3, 3, 0],\n",
       " [0],\n",
       " [3, 3, 3, 0],\n",
       " [1, 2, 2, 3, 0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling of Markov process (4 states)\n",
    "num_episodes = 7\n",
    "list_episodes = []\n",
    "length_episode = 100000 # 최대 에피소드 길이\n",
    "\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    # start of a new episode\n",
    "    episode = []\n",
    "    s = 3 # current state (computer)\n",
    "    t = 0 # episode counter (time)\n",
    "    while (t < length_episode) and (s != 0):\n",
    "        s = next_state(s, P) # 다음 상태를 다시 s로 업데이트\n",
    "        episode.append(s)\n",
    "        t += 1\n",
    "        \n",
    "    list_episodes.append(episode)\n",
    "\n",
    "list_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과에서 보듯이, 일반적으로 각 에피소드의 길이가 달라지게 된다.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
